{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba as jb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jb.suggest_freq('沙瑞金', True)\n",
    "jb.suggest_freq('易学习', True)\n",
    "jb.suggest_freq('王大路', True)\n",
    "jb.suggest_freq('京州', True)\n",
    "jb.suggest_freq('道口县', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "jb.suggest_freq('风生水起', True)\n",
    "with open('nlp_test0_bak.txt', 'r') as f:\n",
    "    file = f.read()\n",
    "    content = ' '.join(jb.cut(file))\n",
    "with open('nlp_test0_bak.txt', 'w') as f:\n",
    "    f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "jb.suggest_freq('欧阳菁', True)\n",
    "with open('nlp_test2_bak.txt', 'r') as f:\n",
    "    file = f.read()\n",
    "    content = ' '.join(jb.cut(file))\n",
    "with open('nlp_test3_bak.txt', 'w') as f:\n",
    "    f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "jb.suggest_freq('桓温', True)\n",
    "jb.suggest_freq('三战三胜', True)\n",
    "with open('nlp_test4_bak.txt', 'r') as f:\n",
    "    file = f.read()\n",
    "    content = ' '.join(jb.cut(file))\n",
    "with open('nlp_test5_bak.txt', 'w') as f:\n",
    "    f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "另 ， 这部 剧 估计 想 告诉 人们 ， 人心 险恶 ， 没有 绝对 的 好人 ， 也 没有 绝对 的 坏人 ， 这个 观点 是 正确 的 ， 但是 此剧 为了 表现 这点 ， 竟然 把 剧中 所有 的 人 都 安排 同样 份量 的 正 能量 和 负 能量 ， 没有 一个 相对 靠谱点 的 人 ， 这 就 过分 了 。 \n",
      "\n",
      "剧中 相互 对立 的 两派 ， 地位 转换 非常 迅速 ， 只是 一支 股票 （ 或 基金 ） 涨跌 就 决定 ， 赢 的 一方 就 好像 流弊 到 天下无敌 ， 输 的 一方 就 沦落 到 被 人 砍杀 ， 这 也 太假 。 就 连 新 韭菜 都 知道 不能 借贷 炒股 ， 这样 至少 不至于 亏太多 。 但是 剧中 人物 全都 借钱 在 赌 ， 这 实在 属于 瞎扯 。 \n",
      "\n",
      "门槛 是 低 ， 但是 也 没有 低到 这样 的 吧 ？ 文凭 没有 你 连 HR 的 眼 都 入 不了 ， 花 了 几万块 去 培训 又 怎么样 ， 伪造 学历 是 可以 ， 但是 查出来 照样 要 被 劝退 ， 稍微 有点 头脑 还 搞 什么 程序员 ， 去 做 销售 不是 更好 ？ 销售 要求 更 低 ， 咋 不 去 做 销售 ？ 非要 来 碰 程序 ？ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('./nlp_test0_bak.txt') as f3:\n",
    "    res1 = f3.read()\n",
    "print(res1)\n",
    "with open('./nlp_test3_bak.txt') as f4:\n",
    "    res2 = f4.read()\n",
    "print(res2)\n",
    "with open('./nlp_test5_bak.txt') as f5:\n",
    "    res3 = f5.read()\n",
    "print(res3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = '/home/liuchao/Downloads/stop_words.txt'\n",
    "stpwrd_dic = open(stop, 'r', encoding='gbk')\n",
    "stpwrd_content = stpwrd_dic.read()\n",
    "#将停用词表转换为list  \n",
    "stpwrdlst = stpwrd_content.splitlines()\n",
    "stpwrd_dic.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA是基于词频统计的，故无需TF-idf\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(stop_words=stpwrdlst)\n",
    "docs = [res1, res2, res3]\n",
    "result = cv.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hr': 0,\n",
       " '一个': 1,\n",
       " '一支': 2,\n",
       " '一方': 3,\n",
       " '两派': 4,\n",
       " '亏太多': 5,\n",
       " '人们': 6,\n",
       " '人心': 7,\n",
       " '人物': 8,\n",
       " '份量': 9,\n",
       " '伪造': 10,\n",
       " '估计': 11,\n",
       " '低到': 12,\n",
       " '借贷': 13,\n",
       " '借钱': 14,\n",
       " '决定': 15,\n",
       " '几万块': 16,\n",
       " '剧中': 17,\n",
       " '劝退': 18,\n",
       " '同样': 19,\n",
       " '告诉': 20,\n",
       " '地位': 21,\n",
       " '坏人': 22,\n",
       " '培训': 23,\n",
       " '基金': 24,\n",
       " '天下无敌': 25,\n",
       " '太假': 26,\n",
       " '头脑': 27,\n",
       " '好人': 28,\n",
       " '好像': 29,\n",
       " '学历': 30,\n",
       " '安排': 31,\n",
       " '实在': 32,\n",
       " '对立': 33,\n",
       " '属于': 34,\n",
       " '所有': 35,\n",
       " '文凭': 36,\n",
       " '更好': 37,\n",
       " '有点': 38,\n",
       " '查出来': 39,\n",
       " '正确': 40,\n",
       " '此剧': 41,\n",
       " '沦落': 42,\n",
       " '流弊': 43,\n",
       " '涨跌': 44,\n",
       " '炒股': 45,\n",
       " '照样': 46,\n",
       " '相互': 47,\n",
       " '相对': 48,\n",
       " '瞎扯': 49,\n",
       " '知道': 50,\n",
       " '砍杀': 51,\n",
       " '程序': 52,\n",
       " '程序员': 53,\n",
       " '稍微': 54,\n",
       " '股票': 55,\n",
       " '能量': 56,\n",
       " '至少': 57,\n",
       " '表现': 58,\n",
       " '要求': 59,\n",
       " '观点': 60,\n",
       " '转换': 61,\n",
       " '迅速': 62,\n",
       " '过分': 63,\n",
       " '这点': 64,\n",
       " '这部': 65,\n",
       " '销售': 66,\n",
       " '门槛': 67,\n",
       " '险恶': 68,\n",
       " '非要': 69,\n",
       " '靠谱点': 70,\n",
       " '韭菜': 71}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02490976, 0.97509024],\n",
       "       [0.98233542, 0.01766458],\n",
       "       [0.0262422 , 0.9737578 ]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_topics=2,\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "docres = lda.fit_transform(result)\n",
    "docres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.87740093, 0.79786907, 1.79610625, 1.6373788 , 1.68303174,\n",
       "        1.23979225, 0.81922801, 0.78070799, 0.80077945, 1.7073741 ,\n",
       "        1.30440478, 0.76337621, 1.10114153, 1.25534974, 1.35409782,\n",
       "        1.15272665, 1.76401171, 0.86132421, 1.1804551 , 1.1198504 ,\n",
       "        0.84656062, 0.84333457, 1.17307355, 1.5963166 , 1.19998236,\n",
       "        1.20398993, 0.68958231, 0.76205863, 1.17231765, 0.79660579,\n",
       "        1.22528933, 0.79796989, 1.1937186 , 1.20814853, 1.17562209,\n",
       "        1.12980797, 0.69452198, 1.24497843, 1.26777989, 1.6955448 ,\n",
       "        0.79512148, 1.1828447 , 0.75846627, 0.78423129, 1.16667043,\n",
       "        0.82543485, 0.81185104, 1.59262859, 1.28739217, 1.21741629,\n",
       "        3.04625393, 1.29557974, 0.79416072, 2.18117845, 0.78940074,\n",
       "        1.697069  , 0.70722256, 2.18096941, 0.7520081 , 2.1032991 ,\n",
       "        1.33403868, 1.26989381, 4.42903895, 0.81932868, 1.26119809,\n",
       "        0.8270106 , 1.24837058, 0.7166318 , 1.23656351, 1.3018813 ,\n",
       "        0.74970701, 2.24974874, 1.1865307 , 1.25685576, 1.19434308,\n",
       "        1.26378252, 1.24253725, 0.71589422, 1.23660152, 0.80913986,\n",
       "        1.17968101, 1.28984071, 1.22264132, 1.32520787],\n",
       "       [1.2140746 , 1.18549239, 0.71026665, 0.74777239, 0.74420818,\n",
       "        0.88928555, 1.15597467, 1.26885167, 1.16791725, 0.69108881,\n",
       "        0.82717029, 1.23939192, 0.71606335, 0.81430931, 0.7315942 ,\n",
       "        0.7332455 , 0.76276756, 1.14742327, 0.80397966, 0.69574073,\n",
       "        1.1615179 , 1.22301664, 0.70262805, 0.80454671, 0.798122  ,\n",
       "        0.76011694, 1.34947246, 1.29797668, 0.75791839, 1.20809775,\n",
       "        0.83080539, 1.25768586, 0.76808003, 0.73041649, 0.79350326,\n",
       "        0.77398401, 1.73205829, 0.79385143, 0.71962494, 0.74874131,\n",
       "        1.35375444, 0.77540346, 1.19692183, 1.70407113, 0.77807894,\n",
       "        1.23333183, 1.59973301, 0.76211226, 0.74618774, 0.79203831,\n",
       "        0.71963104, 0.81214623, 1.24479885, 0.81642476, 1.66937483,\n",
       "        0.76800548, 1.28811568, 0.85442492, 1.25096788, 0.74894645,\n",
       "        0.80691137, 0.81476056, 0.71094061, 1.25472106, 0.73748527,\n",
       "        1.21760576, 0.67118887, 1.23313462, 0.75950778, 0.77413517,\n",
       "        1.3356364 , 0.79856833, 0.79656219, 0.76281096, 0.66695758,\n",
       "        0.88831757, 0.73913135, 1.31072115, 0.71466552, 1.2897128 ,\n",
       "        0.73624794, 0.74337172, 0.9034772 , 0.71360727]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
