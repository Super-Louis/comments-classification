{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import jieba\n",
    "import pandas as pd\n",
    "import pyprind\n",
    "import re\n",
    "import multiprocessing\n",
    "\n",
    "class WordCut():\n",
    "    # process sentence and cut if with jieba\n",
    "    def cut_word(self,n):\n",
    "        inifile = open('../data/split_file00{}'.format(n), 'r')\n",
    "        csvfile = open('../data/csvfile00{}'.format(n), 'w')\n",
    "        writer = csv.writer(csvfile)\n",
    "        for l in inifile:\n",
    "            content = json.loads(l)\n",
    "            score = content['comment_detail']['score']\n",
    "            comment = content['comment_detail']['content']\n",
    "            comment = re.sub('[\\W]+', ' ', comment)\n",
    "            # 分词之后的调整\n",
    "            if not (score and comment.strip()): # 剔除掉空值。错了！ 应该为and，任何一个为空则跳过\n",
    "                continue\n",
    "            if comment[:3] == '此用户':\n",
    "                continue\n",
    "            sentence = jieba.cut(comment)\n",
    "            sentence = ' '.join(sentence)\n",
    "            label = 0 if score <= 3 else 1 # 小于等于3为反例（负面）\n",
    "            writer.writerow([sentence, label])\n",
    "        inifile.close()\n",
    "        csvfile.close()\n",
    "        \n",
    "    def run(self):\n",
    "        ps = list()\n",
    "        for i in range(6):\n",
    "            p = multiprocessing.Process(target=self.cut_word, args=(i,), name=i)\n",
    "            ps.append(p)\n",
    "            p.start()\n",
    "            print(\"process {} start\".format(i))\n",
    "        for p in ps:\n",
    "            p.join()\n",
    "            print(\"process {} joined\".format(p.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process 0 start\n",
      "process 1 start\n",
      "process 2 start\n",
      "process 3 start\n",
      "process 4 start\n",
      "process 5 start\n",
      "process Process-105 joined\n",
      "process 1 joined\n",
      "process 2 joined\n",
      "process 3 joined\n",
      "process 4 joined\n",
      "process 5 joined\n"
     ]
    }
   ],
   "source": [
    "WC = WordCut()\n",
    "WC.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76487, 2)\n",
      "(74097, 2)\n",
      "(75196, 2)\n",
      "(76778, 2)\n",
      "(78202, 2)\n",
      "(40803, 2)\n"
     ]
    }
   ],
   "source": [
    "# save data to local csv file\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "df_all = pd.DataFrame(columns=['sentence', 'label']) # append不同dataFrame时，column应保持一致\n",
    "for i in range(6):\n",
    "    df = pd.read_csv('../data/csvfile00{}'.format(i), delimiter=',')\n",
    "    df.columns = ['sentence', 'label']\n",
    "    print(df.shape)\n",
    "    df_all = df_all.append(df, ignore_index=True)\n",
    "# reindex data\n",
    "df_all = df_all.reindex(np.random.permutation(df_all.index))\n",
    "df_all.to_csv('../data/csvfinal.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                    不能 关 数据   感觉 好 垃圾\n",
       "1                                    手机 便宜   但 声音 有点 小\n",
       "2    用 了 三天   非常 好   还是 htc 不 一样 的 感觉   非常 流畅   昨天 ...\n",
       "3    手机 给 老妈 用 的   使用 后 反馈 还 行   声音 比较 大   字体 清楚   ...\n",
       "4    买 了 一个 星期 多 了   游戏 也 玩 了   没 出现 卡机 现象   屏幕 有 水...\n",
       "Name: sentence, dtype: object"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/csvfinal.csv')\n",
    "# df.drop_duplicates() # 删除重复数据\n",
    "# df.isnull().sum() # 判断空值\n",
    "# df.dropna()\n",
    "# df.sentence.astype==float\n",
    "df['sentence'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load stop_words\n",
    "stopwords = open('../data/stop_words.txt', 'r', encoding='GBK').read() #打不开则用GBK编码， 默认使用utf8\n",
    "stops = stopwords.splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform sentence to list/array\n",
    "count = 0\n",
    "def tokenizer(text):\n",
    "    try:\n",
    "        return text.split() # 以任意空格分割句子\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(text)\n",
    "        global count\n",
    "        count += 1\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据分为测试集与训练集\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X = df.iloc[:, 0].values\n",
    "y = df.iloc[:,1].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['之前 的 坏 了   买 了 一个 先用 用   屏幕 看上去 不像 正品 的   耗电量 巨快', '服务 差劲',\n",
       "       '非常 不 满意 的 一次 网购   买 之前 看 评论   普遍 都 觉得 还 不错   而且 是 给 老人 买   我 觉得 这些 配置 应该 可以 了   可 手机 拿到 半天 不到   按键 迟钝   滑动 缓慢   应用 点不开   客服 给 我 的 官方 解释 是   手机 需要 垃圾 清理   我 想 请问   新 到手 的 机子 就 开始 清 垃圾   这能 用 几天   这 还要 在 我 能点 进去 的 情况 下  ',\n",
       "       ...,\n",
       "       '手机 有 问题   不仅 不 退款   而且 也 不 给 退换   找 了 个 手机 有 划痕 的 借口   退返 厂家 检测 快递费 还是 自付 的   最后 还是 退 给 我 之前 有 问题 的 手机   希望 大家 别 上当   换家 靠 谱 的 购物 消费 吧',\n",
       "       '飞远 快递 配送 的   不能 送上门   叫 老人家 跑 10 公里 远 地方 去 拿   投诉 后 还 打电话 来 骂',\n",
       "       '卡成 狗 了   玩 游戏卡 到 爆   你妹 的   16G 内存   竟然 打 不了 王者   等 放出 技能 人 都 死 啦  '],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[y_train==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed: 149.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=None, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "  ...nalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid=[{'vect__ngram_range': [(1, 1)], 'vect__norm': ['l1', 'l2'], 'clf__penalty': ['l1', 'l2'], 'clf__C': [1.0, 10.0, 100.0]}, {'vect__ngram_range': [(1, 1)], 'vect__use_idf': [False], 'vect__norm': [None], 'clf__penalty': ['l1', 'l2'], 'clf__C': [1.0, 10.0, 100.0]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='accuracy', verbose=-1)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "clf = LogisticRegression(random_state=0)\n",
    "tfidf = TfidfVectorizer(preprocessor=None, lowercase=None, strip_accents=None, stop_words=stops, tokenizer=tokenizer)\n",
    "lr_tfidf = Pipeline([('vect', tfidf), ('clf', clf)])\n",
    "params_grid = [{'vect__ngram_range': [(1,1)],\n",
    "      'vect__norm': ['l1', 'l2'], # 数据归一化\n",
    "      'clf__penalty': ['l1', 'l2'], # logistic回归罚项\n",
    "      'clf__C': [1.0, 10.0, 100.0]\n",
    "     },                  \n",
    "    {'vect__ngram_range': [(1,1)],\n",
    "     'vect__use_idf': [False], # 不使用idf\n",
    "     'vect__norm': [None],\n",
    "     'clf__penalty': ['l1', 'l2'],\n",
    "     'clf__C': [1.0, 10.0, 100.0]}]\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, params_grid, scoring='accuracy', cv=5, verbose=-1, n_jobs=-1)\n",
    "gs_lr_tfidf.fit(X_train, y_train)\n",
    "# matrix = tfidf.fit_transform(X_test)\n",
    "# vocdict = tfidf.vocabulary_\n",
    "# invers_dict = {v: k for k, v in vocdict.items()}\n",
    "# print(matrix)\n",
    "# print(invers_dict[11745])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9586504639199712"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_lr_tfidf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = gs_lr_tfidf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=None, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "  ...nalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9583138951047292"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(['手机 开机 很 慢，反应 很 慢'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "y_train.count(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'count'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-217-35b5a1736c12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'count'"
     ]
    }
   ],
   "source": [
    "print(y_train.count(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
