{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import jieba\n",
    "import pandas as pd\n",
    "import pyprind\n",
    "import re\n",
    "import multiprocessing\n",
    "\n",
    "class WordCut():\n",
    "    # process sentence and cut if with jieba\n",
    "    def cut_word(self,n):\n",
    "        inifile = open('../data/split_file00{}'.format(n), 'r')\n",
    "        csvfile = open('../data/csvfile00{}'.format(n), 'w')\n",
    "        writer = csv.writer(csvfile)\n",
    "        for l in inifile:\n",
    "            content = json.loads(l)\n",
    "            score = content['comment_detail']['score']\n",
    "            comment = content['comment_detail']['content']\n",
    "            comment = re.sub('[\\W]+', ' ', comment)\n",
    "            # 分词之后的调整\n",
    "            if not (score and comment.strip()): # 剔除掉空值。错了！ 应该为and，任何一个为空则跳过\n",
    "                continue\n",
    "            if comment[:3] == '此用户':\n",
    "                continue\n",
    "            sentence = jieba.cut(comment)\n",
    "            sentence = ' '.join(sentence)\n",
    "            label = 0 if score <= 3 else 1 # 小于等于3为反例（负面）\n",
    "            writer.writerow([sentence, label])\n",
    "        inifile.close()\n",
    "        csvfile.close()\n",
    "        \n",
    "    def run(self):\n",
    "        ps = list()\n",
    "        for i in range(6):\n",
    "            p = multiprocessing.Process(target=self.cut_word, args=(i,), name=i)\n",
    "            ps.append(p)\n",
    "            p.start()\n",
    "            print(\"process {} start\".format(i))\n",
    "        for p in ps:\n",
    "            p.join()\n",
    "            print(\"process {} joined\".format(p.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process 0 start\n",
      "process 1 start\n",
      "process 2 start\n",
      "process 3 start\n",
      "process 4 start\n",
      "process 5 start\n",
      "process Process-105 joined\n",
      "process 1 joined\n",
      "process 2 joined\n",
      "process 3 joined\n",
      "process 4 joined\n",
      "process 5 joined\n"
     ]
    }
   ],
   "source": [
    "WC = WordCut()\n",
    "WC.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76487, 2)\n",
      "(74097, 2)\n",
      "(75196, 2)\n",
      "(76778, 2)\n",
      "(78202, 2)\n",
      "(40803, 2)\n"
     ]
    }
   ],
   "source": [
    "# save data to local csv file\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "df_all = pd.DataFrame(columns=['sentence', 'label']) # append不同dataFrame时，column应保持一致\n",
    "for i in range(6):\n",
    "    df = pd.read_csv('../data/csvfile00{}'.format(i), delimiter=',')\n",
    "    df.columns = ['sentence', 'label']\n",
    "    print(df.shape)\n",
    "    df_all = df_all.append(df, ignore_index=True)\n",
    "# reindex data\n",
    "df_all = df_all.reindex(np.random.permutation(df_all.index))\n",
    "df_all.to_csv('../data/csvfinal.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                    不能 关 数据   感觉 好 垃圾\n",
       "1                                    手机 便宜   但 声音 有点 小\n",
       "2    用 了 三天   非常 好   还是 htc 不 一样 的 感觉   非常 流畅   昨天 ...\n",
       "3    手机 给 老妈 用 的   使用 后 反馈 还 行   声音 比较 大   字体 清楚   ...\n",
       "4    买 了 一个 星期 多 了   游戏 也 玩 了   没 出现 卡机 现象   屏幕 有 水...\n",
       "Name: sentence, dtype: object"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import jieba\n",
    "import pandas as pd\n",
    "df = pd.read_csv('../data/csvfinal.csv')\n",
    "# df.drop_duplicates() # 删除重复数据\n",
    "# df.isnull().sum() # 判断空值\n",
    "# df.dropna()\n",
    "# df.sentence.astype==float\n",
    "df['sentence'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load stop_words\n",
    "stopwords = open('../data/stop_words.txt', 'r', encoding='GBK').read() #打不开则用GBK编码， 默认使用utf8\n",
    "stops = stopwords.splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform sentence to list/array\n",
    "count = 0\n",
    "def tokenizer(text):\n",
    "    try:\n",
    "        return text.split() # 以任意空格分割句子\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(text)\n",
    "        global count\n",
    "        count += 1\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liuchao/.pyenv/versions/3.6.3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# 将数据分为测试集与训练集\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X = df.iloc[:, 0].values\n",
    "y = df.iloc[:,1].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['之前 的 坏 了   买 了 一个 先用 用   屏幕 看上去 不像 正品 的   耗电量 巨快', '服务 差劲',\n",
       "       '非常 不 满意 的 一次 网购   买 之前 看 评论   普遍 都 觉得 还 不错   而且 是 给 老人 买   我 觉得 这些 配置 应该 可以 了   可 手机 拿到 半天 不到   按键 迟钝   滑动 缓慢   应用 点不开   客服 给 我 的 官方 解释 是   手机 需要 垃圾 清理   我 想 请问   新 到手 的 机子 就 开始 清 垃圾   这能 用 几天   这 还要 在 我 能点 进去 的 情况 下  ',\n",
       "       ...,\n",
       "       '手机 有 问题   不仅 不 退款   而且 也 不 给 退换   找 了 个 手机 有 划痕 的 借口   退返 厂家 检测 快递费 还是 自付 的   最后 还是 退 给 我 之前 有 问题 的 手机   希望 大家 别 上当   换家 靠 谱 的 购物 消费 吧',\n",
       "       '飞远 快递 配送 的   不能 送上门   叫 老人家 跑 10 公里 远 地方 去 拿   投诉 后 还 打电话 来 骂',\n",
       "       '卡成 狗 了   玩 游戏卡 到 爆   你妹 的   16G 内存   竟然 打 不了 王者   等 放出 技能 人 都 死 啦  '],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[y_train==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:  3.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=None, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "  ..., max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,\n",
       "     verbose=0))]),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid=[{'vect__ngram_range': [(1, 1)], 'vect__norm': ['l2'], 'clf__C': [0.01, 1, 10]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='roc_auc', verbose=-1)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score, make_scorer, f1_score, accuracy_score\n",
    "clf = LinearSVC(random_state=0, class_weight='balanced')\n",
    "tfidf = TfidfVectorizer(preprocessor=None, lowercase=None, strip_accents=None, stop_words=stops, tokenizer=tokenizer)\n",
    "svm_tfidf = Pipeline([('vect', tfidf), ('clf', clf)])\n",
    "params_grid = [{'vect__ngram_range': [(1,1)],\n",
    "      'vect__norm': ['l2'], # 数据归一化\n",
    "      'clf__C': [0.01, 1, 10]}\n",
    "#                {'vect__ngram_range': [(1,1)],\n",
    "#       'vect__norm': ['l2'], # 数据归一化\n",
    "#       'clf__kernel': ['rbf'],\n",
    "#       'clf__gamma':[0.01, 0.1, 10],\n",
    "#       'clf__C': [0.001, 0.1,10.0]\n",
    "#      }\n",
    "]\n",
    "def tp(y_true, y_pred): return confusion_matrix(y_true, y_pred)[0, 0]\n",
    "def tn(y_true, y_pred): return confusion_matrix(y_true, y_pred)[0, 0]\n",
    "def fp(y_true, y_pred): return confusion_matrix(y_true, y_pred)[1, 0]\n",
    "def fn(y_true, y_pred): return confusion_matrix(y_true, y_pred)[0, 1]\n",
    "scorer = make_scorer(f1_score, pos_label=0) # 类别不均衡问题，指定0为正例， f1_score, 准确率与召回率的组合\n",
    "gs_lr_tfidf = GridSearchCV(svm_tfidf, params_grid, scoring='roc_auc', cv=5, verbose=-1, n_jobs=-1)\n",
    "gs_lr_tfidf.fit(X_train, y_train)\n",
    "# matrix = tfidf.fit_transform(X_test)\n",
    "# vocdict = tfidf.vocabulary_\n",
    "# invers_dict = {v: k for k, v in vocdict.items()}\n",
    "# print(matrix)\n",
    "# print(invers_dict[11745])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9288358775172147"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_lr_tfidf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__C': 0.01, 'vect__ngram_range': (1, 1), 'vect__norm': 'l2'}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_lr_tfidf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = gs_lr_tfidf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=None, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "  ..., max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,\n",
       "     verbose=0))])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  5288,    730],\n",
       "       [ 18050, 102401]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = clf.predict(X_test)\n",
    "confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "confmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(['手机 分辨率 差'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8515051119246614"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict = clf.predict(X_test)\n",
    "accuracy_score(y_predict, y_test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "y_train.count(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.count(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "pickle.dump(clf, open('linear_0.85.pkl', 'wb'), protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=None, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "  ...nalty='l2', random_state=0,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = pickle.load(open('linear_0.85.pkl', 'rb'))\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf': LogisticRegression(C=0.1, class_weight='balanced', dual=False,\n",
       "           fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "           multi_class='ovr', n_jobs=1, penalty='l2', random_state=0,\n",
       "           solver='liblinear', tol=0.0001, verbose=0, warm_start=False),\n",
       " 'clf__C': 0.1,\n",
       " 'clf__class_weight': 'balanced',\n",
       " 'clf__dual': False,\n",
       " 'clf__fit_intercept': True,\n",
       " 'clf__intercept_scaling': 1,\n",
       " 'clf__max_iter': 100,\n",
       " 'clf__multi_class': 'ovr',\n",
       " 'clf__n_jobs': 1,\n",
       " 'clf__penalty': 'l2',\n",
       " 'clf__random_state': 0,\n",
       " 'clf__solver': 'liblinear',\n",
       " 'clf__tol': 0.0001,\n",
       " 'clf__verbose': 0,\n",
       " 'clf__warm_start': False,\n",
       " 'memory': None,\n",
       " 'steps': [('vect',\n",
       "   TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=None, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=[',', '?', '、', '。', '“', '”', '《', '》', '！', '，', '：', '；', '？', '人民', '末##末', '啊', '阿', '哎', '哎呀', '哎哟', '唉', '俺', '俺们', '按', '按照', '吧', '吧哒', '把', '罢了', '被', '本', '本着', '比', '比方', '比如', '鄙人', '彼', '彼此', '边', '别', '别的', '别说', '并', '并且', '不比', '不成', '不单', '不但', '不独', '不管', '不光', '不过', '不...如期', '如前所述', '如上', '如下', '汝', '三番两次', '三番五次', '三天两头', '瑟瑟', '沙沙', '上', '上来', '上去', '爸爸', '妈妈', '看着'],\n",
       "           strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=<function tokenizer at 0x7f1c085f8488>, use_idf=True,\n",
       "           vocabulary=None)),\n",
       "  ('clf', LogisticRegression(C=0.1, class_weight='balanced', dual=False,\n",
       "             fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "             multi_class='ovr', n_jobs=1, penalty='l2', random_state=0,\n",
       "             solver='liblinear', tol=0.0001, verbose=0, warm_start=False))],\n",
       " 'vect': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=None, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=[',', '?', '、', '。', '“', '”', '《', '》', '！', '，', '：', '；', '？', '人民', '末##末', '啊', '阿', '哎', '哎呀', '哎哟', '唉', '俺', '俺们', '按', '按照', '吧', '吧哒', '把', '罢了', '被', '本', '本着', '比', '比方', '比如', '鄙人', '彼', '彼此', '边', '别', '别的', '别说', '并', '并且', '不比', '不成', '不单', '不但', '不独', '不管', '不光', '不过', '不...如期', '如前所述', '如上', '如下', '汝', '三番两次', '三番五次', '三天两头', '瑟瑟', '沙沙', '上', '上来', '上去', '爸爸', '妈妈', '看着'],\n",
       "         strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=<function tokenizer at 0x7f1c085f8488>, use_idf=True,\n",
       "         vocabulary=None),\n",
       " 'vect__analyzer': 'word',\n",
       " 'vect__binary': False,\n",
       " 'vect__decode_error': 'strict',\n",
       " 'vect__dtype': numpy.int64,\n",
       " 'vect__encoding': 'utf-8',\n",
       " 'vect__input': 'content',\n",
       " 'vect__lowercase': None,\n",
       " 'vect__max_df': 1.0,\n",
       " 'vect__max_features': None,\n",
       " 'vect__min_df': 1,\n",
       " 'vect__ngram_range': (1, 1),\n",
       " 'vect__norm': 'l2',\n",
       " 'vect__preprocessor': None,\n",
       " 'vect__smooth_idf': True,\n",
       " 'vect__stop_words': [',',\n",
       "  '?',\n",
       "  '、',\n",
       "  '。',\n",
       "  '“',\n",
       "  '”',\n",
       "  '《',\n",
       "  '》',\n",
       "  '！',\n",
       "  '，',\n",
       "  '：',\n",
       "  '；',\n",
       "  '？',\n",
       "  '人民',\n",
       "  '末##末',\n",
       "  '啊',\n",
       "  '阿',\n",
       "  '哎',\n",
       "  '哎呀',\n",
       "  '哎哟',\n",
       "  '唉',\n",
       "  '俺',\n",
       "  '俺们',\n",
       "  '按',\n",
       "  '按照',\n",
       "  '吧',\n",
       "  '吧哒',\n",
       "  '把',\n",
       "  '罢了',\n",
       "  '被',\n",
       "  '本',\n",
       "  '本着',\n",
       "  '比',\n",
       "  '比方',\n",
       "  '比如',\n",
       "  '鄙人',\n",
       "  '彼',\n",
       "  '彼此',\n",
       "  '边',\n",
       "  '别',\n",
       "  '别的',\n",
       "  '别说',\n",
       "  '并',\n",
       "  '并且',\n",
       "  '不比',\n",
       "  '不成',\n",
       "  '不单',\n",
       "  '不但',\n",
       "  '不独',\n",
       "  '不管',\n",
       "  '不光',\n",
       "  '不过',\n",
       "  '不仅',\n",
       "  '不拘',\n",
       "  '不论',\n",
       "  '不怕',\n",
       "  '不然',\n",
       "  '不如',\n",
       "  '不特',\n",
       "  '不惟',\n",
       "  '不问',\n",
       "  '不只',\n",
       "  '朝',\n",
       "  '朝着',\n",
       "  '趁',\n",
       "  '趁着',\n",
       "  '乘',\n",
       "  '冲',\n",
       "  '除',\n",
       "  '除此之外',\n",
       "  '除非',\n",
       "  '除了',\n",
       "  '此',\n",
       "  '此间',\n",
       "  '此外',\n",
       "  '从',\n",
       "  '从而',\n",
       "  '打',\n",
       "  '待',\n",
       "  '但',\n",
       "  '但是',\n",
       "  '当',\n",
       "  '当着',\n",
       "  '到',\n",
       "  '得',\n",
       "  '的',\n",
       "  '的话',\n",
       "  '等',\n",
       "  '等等',\n",
       "  '地',\n",
       "  '第',\n",
       "  '叮咚',\n",
       "  '对',\n",
       "  '对于',\n",
       "  '多',\n",
       "  '多少',\n",
       "  '而',\n",
       "  '而况',\n",
       "  '而且',\n",
       "  '而是',\n",
       "  '而外',\n",
       "  '而言',\n",
       "  '而已',\n",
       "  '尔后',\n",
       "  '反过来',\n",
       "  '反过来说',\n",
       "  '反之',\n",
       "  '非但',\n",
       "  '非徒',\n",
       "  '否则',\n",
       "  '嘎',\n",
       "  '嘎登',\n",
       "  '该',\n",
       "  '赶',\n",
       "  '个',\n",
       "  '各',\n",
       "  '各个',\n",
       "  '各位',\n",
       "  '各种',\n",
       "  '各自',\n",
       "  '给',\n",
       "  '根据',\n",
       "  '跟',\n",
       "  '故',\n",
       "  '故此',\n",
       "  '固然',\n",
       "  '关于',\n",
       "  '管',\n",
       "  '归',\n",
       "  '果然',\n",
       "  '果真',\n",
       "  '过',\n",
       "  '哈',\n",
       "  '哈哈',\n",
       "  '呵',\n",
       "  '和',\n",
       "  '何',\n",
       "  '何处',\n",
       "  '何况',\n",
       "  '何时',\n",
       "  '嘿',\n",
       "  '哼',\n",
       "  '哼唷',\n",
       "  '呼哧',\n",
       "  '乎',\n",
       "  '哗',\n",
       "  '还是',\n",
       "  '还有',\n",
       "  '换句话说',\n",
       "  '换言之',\n",
       "  '或',\n",
       "  '或是',\n",
       "  '或者',\n",
       "  '极了',\n",
       "  '及',\n",
       "  '及其',\n",
       "  '及至',\n",
       "  '即',\n",
       "  '即便',\n",
       "  '即或',\n",
       "  '即令',\n",
       "  '即若',\n",
       "  '即使',\n",
       "  '几',\n",
       "  '几时',\n",
       "  '己',\n",
       "  '既',\n",
       "  '既然',\n",
       "  '既是',\n",
       "  '继而',\n",
       "  '加之',\n",
       "  '假如',\n",
       "  '假若',\n",
       "  '假使',\n",
       "  '鉴于',\n",
       "  '将',\n",
       "  '较',\n",
       "  '较之',\n",
       "  '叫',\n",
       "  '接着',\n",
       "  '结果',\n",
       "  '借',\n",
       "  '紧接着',\n",
       "  '进而',\n",
       "  '尽',\n",
       "  '尽管',\n",
       "  '经',\n",
       "  '经过',\n",
       "  '就',\n",
       "  '就是',\n",
       "  '就是说',\n",
       "  '据',\n",
       "  '具体地说',\n",
       "  '具体说来',\n",
       "  '开始',\n",
       "  '开外',\n",
       "  '靠',\n",
       "  '咳',\n",
       "  '可',\n",
       "  '可见',\n",
       "  '可是',\n",
       "  '可以',\n",
       "  '况且',\n",
       "  '啦',\n",
       "  '来',\n",
       "  '来着',\n",
       "  '离',\n",
       "  '例如',\n",
       "  '哩',\n",
       "  '连',\n",
       "  '连同',\n",
       "  '两者',\n",
       "  '了',\n",
       "  '临',\n",
       "  '另',\n",
       "  '另外',\n",
       "  '另一方面',\n",
       "  '论',\n",
       "  '嘛',\n",
       "  '吗',\n",
       "  '慢说',\n",
       "  '漫说',\n",
       "  '冒',\n",
       "  '么',\n",
       "  '每',\n",
       "  '每当',\n",
       "  '们',\n",
       "  '莫若',\n",
       "  '某',\n",
       "  '某个',\n",
       "  '某些',\n",
       "  '拿',\n",
       "  '哪',\n",
       "  '哪边',\n",
       "  '哪儿',\n",
       "  '哪个',\n",
       "  '哪里',\n",
       "  '哪年',\n",
       "  '哪怕',\n",
       "  '哪天',\n",
       "  '哪些',\n",
       "  '哪样',\n",
       "  '那',\n",
       "  '那边',\n",
       "  '那儿',\n",
       "  '那个',\n",
       "  '那会儿',\n",
       "  '那里',\n",
       "  '那么',\n",
       "  '那么些',\n",
       "  '那么样',\n",
       "  '那时',\n",
       "  '那些',\n",
       "  '那样',\n",
       "  '乃',\n",
       "  '乃至',\n",
       "  '呢',\n",
       "  '能',\n",
       "  '你',\n",
       "  '你们',\n",
       "  '您',\n",
       "  '宁',\n",
       "  '宁可',\n",
       "  '宁肯',\n",
       "  '宁愿',\n",
       "  '哦',\n",
       "  '呕',\n",
       "  '啪达',\n",
       "  '旁人',\n",
       "  '呸',\n",
       "  '凭',\n",
       "  '凭借',\n",
       "  '其',\n",
       "  '其次',\n",
       "  '其二',\n",
       "  '其他',\n",
       "  '其它',\n",
       "  '其一',\n",
       "  '其余',\n",
       "  '其中',\n",
       "  '起',\n",
       "  '起见',\n",
       "  '岂但',\n",
       "  '恰恰相反',\n",
       "  '前后',\n",
       "  '前者',\n",
       "  '且',\n",
       "  '然而',\n",
       "  '然后',\n",
       "  '然则',\n",
       "  '让',\n",
       "  '人家',\n",
       "  '任',\n",
       "  '任何',\n",
       "  '任凭',\n",
       "  '如',\n",
       "  '如此',\n",
       "  '如果',\n",
       "  '如何',\n",
       "  '如其',\n",
       "  '如若',\n",
       "  '如上所述',\n",
       "  '若',\n",
       "  '若非',\n",
       "  '若是',\n",
       "  '啥',\n",
       "  '上下',\n",
       "  '尚且',\n",
       "  '设若',\n",
       "  '设使',\n",
       "  '甚而',\n",
       "  '甚么',\n",
       "  '甚至',\n",
       "  '省得',\n",
       "  '时候',\n",
       "  '什么',\n",
       "  '什么样',\n",
       "  '使得',\n",
       "  '是',\n",
       "  '是的',\n",
       "  '首先',\n",
       "  '谁',\n",
       "  '谁知',\n",
       "  '顺',\n",
       "  '顺着',\n",
       "  '似的',\n",
       "  '虽',\n",
       "  '虽然',\n",
       "  '虽说',\n",
       "  '虽则',\n",
       "  '随',\n",
       "  '随着',\n",
       "  '所',\n",
       "  '所以',\n",
       "  '他',\n",
       "  '他们',\n",
       "  '他人',\n",
       "  '它',\n",
       "  '它们',\n",
       "  '她',\n",
       "  '她们',\n",
       "  '倘',\n",
       "  '倘或',\n",
       "  '倘然',\n",
       "  '倘若',\n",
       "  '倘使',\n",
       "  '腾',\n",
       "  '替',\n",
       "  '通过',\n",
       "  '同',\n",
       "  '同时',\n",
       "  '哇',\n",
       "  '万一',\n",
       "  '往',\n",
       "  '望',\n",
       "  '为',\n",
       "  '为何',\n",
       "  '为了',\n",
       "  '为什么',\n",
       "  '为着',\n",
       "  '喂',\n",
       "  '嗡嗡',\n",
       "  '我',\n",
       "  '我们',\n",
       "  '呜',\n",
       "  '呜呼',\n",
       "  '乌乎',\n",
       "  '无论',\n",
       "  '无宁',\n",
       "  '毋宁',\n",
       "  '嘻',\n",
       "  '吓',\n",
       "  '相对而言',\n",
       "  '像',\n",
       "  '向',\n",
       "  '向着',\n",
       "  '嘘',\n",
       "  '呀',\n",
       "  '焉',\n",
       "  '沿',\n",
       "  '沿着',\n",
       "  '要',\n",
       "  '要不',\n",
       "  '要不然',\n",
       "  '要不是',\n",
       "  '要么',\n",
       "  '要是',\n",
       "  '也',\n",
       "  '也罢',\n",
       "  '也好',\n",
       "  '一',\n",
       "  '一般',\n",
       "  '一旦',\n",
       "  '一方面',\n",
       "  '一来',\n",
       "  '一切',\n",
       "  '一样',\n",
       "  '一则',\n",
       "  '依',\n",
       "  '依照',\n",
       "  '矣',\n",
       "  '以',\n",
       "  '以便',\n",
       "  '以及',\n",
       "  '以免',\n",
       "  '以至',\n",
       "  '以至于',\n",
       "  '以致',\n",
       "  '抑或',\n",
       "  '因',\n",
       "  '因此',\n",
       "  '因而',\n",
       "  '因为',\n",
       "  '哟',\n",
       "  '用',\n",
       "  '由',\n",
       "  '由此可见',\n",
       "  '由于',\n",
       "  '有',\n",
       "  '有的',\n",
       "  '有关',\n",
       "  '有些',\n",
       "  '又',\n",
       "  '于',\n",
       "  '于是',\n",
       "  '于是乎',\n",
       "  '与',\n",
       "  '与此同时',\n",
       "  '与否',\n",
       "  '与其',\n",
       "  '越是',\n",
       "  '云云',\n",
       "  '哉',\n",
       "  '再说',\n",
       "  '再者',\n",
       "  '在',\n",
       "  '在下',\n",
       "  '咱',\n",
       "  '咱们',\n",
       "  '则',\n",
       "  '怎',\n",
       "  '怎么',\n",
       "  '怎么办',\n",
       "  '怎么样',\n",
       "  '怎样',\n",
       "  '咋',\n",
       "  '照',\n",
       "  '照着',\n",
       "  '者',\n",
       "  '这',\n",
       "  '这边',\n",
       "  '这儿',\n",
       "  '这个',\n",
       "  '这会儿',\n",
       "  '这就是说',\n",
       "  '这里',\n",
       "  '这么',\n",
       "  '这么点儿',\n",
       "  '这么些',\n",
       "  '这么样',\n",
       "  '这时',\n",
       "  '这些',\n",
       "  '这样',\n",
       "  '正如',\n",
       "  '吱',\n",
       "  '之',\n",
       "  '之类',\n",
       "  '之所以',\n",
       "  '之一',\n",
       "  '只是',\n",
       "  '只限',\n",
       "  '只要',\n",
       "  '只有',\n",
       "  '至',\n",
       "  '至于',\n",
       "  '诸位',\n",
       "  '着',\n",
       "  '着呢',\n",
       "  '自',\n",
       "  '自从',\n",
       "  '自个儿',\n",
       "  '自各儿',\n",
       "  '自己',\n",
       "  '自家',\n",
       "  '自身',\n",
       "  '综上所述',\n",
       "  '总的来看',\n",
       "  '总的来说',\n",
       "  '总的说来',\n",
       "  '总而言之',\n",
       "  '总之',\n",
       "  '纵',\n",
       "  '纵令',\n",
       "  '纵然',\n",
       "  '纵使',\n",
       "  '遵照',\n",
       "  '作为',\n",
       "  '兮',\n",
       "  '呃',\n",
       "  '呗',\n",
       "  '咚',\n",
       "  '咦',\n",
       "  '喏',\n",
       "  '啐',\n",
       "  '喔唷',\n",
       "  '嗬',\n",
       "  '嗯',\n",
       "  '嗳',\n",
       "  '~',\n",
       "  '!',\n",
       "  '.',\n",
       "  ':',\n",
       "  '\"',\n",
       "  \"'\",\n",
       "  '(',\n",
       "  ')',\n",
       "  '*',\n",
       "  'A',\n",
       "  '白',\n",
       "  '社会主义',\n",
       "  '--',\n",
       "  '..',\n",
       "  '>>',\n",
       "  ' [',\n",
       "  ' ]',\n",
       "  '',\n",
       "  '<',\n",
       "  '>',\n",
       "  '/',\n",
       "  '\\\\',\n",
       "  '|',\n",
       "  '-',\n",
       "  '_',\n",
       "  '+',\n",
       "  '=',\n",
       "  '&',\n",
       "  '^',\n",
       "  '%',\n",
       "  '#',\n",
       "  '@',\n",
       "  '`',\n",
       "  ';',\n",
       "  '$',\n",
       "  '（',\n",
       "  '）',\n",
       "  '——',\n",
       "  '—',\n",
       "  '￥',\n",
       "  '·',\n",
       "  '...',\n",
       "  '‘',\n",
       "  '’',\n",
       "  '〉',\n",
       "  '〈',\n",
       "  '…',\n",
       "  '\\u3000',\n",
       "  '0',\n",
       "  '1',\n",
       "  '2',\n",
       "  '3',\n",
       "  '4',\n",
       "  '5',\n",
       "  '6',\n",
       "  '7',\n",
       "  '8',\n",
       "  '9',\n",
       "  '０',\n",
       "  '１',\n",
       "  '２',\n",
       "  '３',\n",
       "  '４',\n",
       "  '５',\n",
       "  '６',\n",
       "  '７',\n",
       "  '８',\n",
       "  '９',\n",
       "  '二',\n",
       "  '三',\n",
       "  '四',\n",
       "  '五',\n",
       "  '六',\n",
       "  '七',\n",
       "  '八',\n",
       "  '九',\n",
       "  '零',\n",
       "  '＞',\n",
       "  '＜',\n",
       "  '＠',\n",
       "  '＃',\n",
       "  '＄',\n",
       "  '％',\n",
       "  '︿',\n",
       "  '＆',\n",
       "  '＊',\n",
       "  '＋',\n",
       "  '～',\n",
       "  '｜',\n",
       "  '［',\n",
       "  '］',\n",
       "  '｛',\n",
       "  '｝',\n",
       "  '啊哈',\n",
       "  '啊呀',\n",
       "  '啊哟',\n",
       "  '挨次',\n",
       "  '挨个',\n",
       "  '挨家挨户',\n",
       "  '挨门挨户',\n",
       "  '挨门逐户',\n",
       "  '挨着',\n",
       "  '按理',\n",
       "  '按期',\n",
       "  '按时',\n",
       "  '按说',\n",
       "  '暗地里',\n",
       "  '暗中',\n",
       "  '暗自',\n",
       "  '昂然',\n",
       "  '八成',\n",
       "  '白白',\n",
       "  '半',\n",
       "  '梆',\n",
       "  '保管',\n",
       "  '保险',\n",
       "  '饱',\n",
       "  '背地里',\n",
       "  '背靠背',\n",
       "  '倍感',\n",
       "  '倍加',\n",
       "  '本人',\n",
       "  '本身',\n",
       "  '甭',\n",
       "  '比起',\n",
       "  '比如说',\n",
       "  '比照',\n",
       "  '毕竟',\n",
       "  '必',\n",
       "  '必定',\n",
       "  '必将',\n",
       "  '必须',\n",
       "  '便',\n",
       "  '别人',\n",
       "  '并非',\n",
       "  '并肩',\n",
       "  '并没',\n",
       "  '并没有',\n",
       "  '并排',\n",
       "  '并无',\n",
       "  '勃然',\n",
       "  '不',\n",
       "  '不必',\n",
       "  '不常',\n",
       "  '不大',\n",
       "  '不但...而且',\n",
       "  '不得',\n",
       "  '不得不',\n",
       "  '不得了',\n",
       "  '不得已',\n",
       "  '不迭',\n",
       "  '不定',\n",
       "  '不对',\n",
       "  '不妨',\n",
       "  '不管怎样',\n",
       "  '不会',\n",
       "  '不仅...而且',\n",
       "  '不仅仅',\n",
       "  '不仅仅是',\n",
       "  '不经意',\n",
       "  '不可开交',\n",
       "  '不可抗拒',\n",
       "  '不力',\n",
       "  '不了',\n",
       "  '不料',\n",
       "  '不满',\n",
       "  '不免',\n",
       "  '不能不',\n",
       "  '不起',\n",
       "  '不巧',\n",
       "  '不然的话',\n",
       "  '不日',\n",
       "  '不少',\n",
       "  '不胜',\n",
       "  '不时',\n",
       "  '不是',\n",
       "  '不同',\n",
       "  '不能',\n",
       "  '不要',\n",
       "  '不外',\n",
       "  '不外乎',\n",
       "  '不下',\n",
       "  '不限',\n",
       "  '不消',\n",
       "  '不已',\n",
       "  '不亦乐乎',\n",
       "  '不由得',\n",
       "  '不再',\n",
       "  '不择手段',\n",
       "  '不怎么',\n",
       "  '不曾',\n",
       "  '不知不觉',\n",
       "  '不止',\n",
       "  '不止一次',\n",
       "  '不至于',\n",
       "  '才',\n",
       "  '才能',\n",
       "  '策略地',\n",
       "  '差不多',\n",
       "  '差一点',\n",
       "  '常',\n",
       "  '常常',\n",
       "  '常言道',\n",
       "  '常言说',\n",
       "  '常言说得好',\n",
       "  '长此下去',\n",
       "  '长话短说',\n",
       "  '长期以来',\n",
       "  '长线',\n",
       "  '敞开儿',\n",
       "  '彻夜',\n",
       "  '陈年',\n",
       "  '趁便',\n",
       "  '趁机',\n",
       "  '趁热',\n",
       "  '趁势',\n",
       "  '趁早',\n",
       "  '成年',\n",
       "  '成年累月',\n",
       "  '成心',\n",
       "  '乘机',\n",
       "  '乘胜',\n",
       "  '乘势',\n",
       "  '乘隙',\n",
       "  '乘虚',\n",
       "  '诚然',\n",
       "  '迟早',\n",
       "  '充分',\n",
       "  '充其极',\n",
       "  '充其量',\n",
       "  '抽冷子',\n",
       "  '臭',\n",
       "  '初',\n",
       "  '出',\n",
       "  '出来',\n",
       "  '出去',\n",
       "  '除此',\n",
       "  '除此而外',\n",
       "  '除此以外',\n",
       "  '除开',\n",
       "  '除去',\n",
       "  '除却',\n",
       "  '除外',\n",
       "  '处处',\n",
       "  '川流不息',\n",
       "  '传',\n",
       "  '传说',\n",
       "  '传闻',\n",
       "  '串行',\n",
       "  '纯',\n",
       "  '纯粹',\n",
       "  '此后',\n",
       "  '此中',\n",
       "  '次第',\n",
       "  '匆匆',\n",
       "  '从不',\n",
       "  '从此',\n",
       "  '从此以后',\n",
       "  '从古到今',\n",
       "  '从古至今',\n",
       "  '从今以后',\n",
       "  '从宽',\n",
       "  '从来',\n",
       "  '从轻',\n",
       "  '从速',\n",
       "  '从头',\n",
       "  '从未',\n",
       "  '从无到有',\n",
       "  '从小',\n",
       "  '从新',\n",
       "  '从严',\n",
       "  '从优',\n",
       "  '从早到晚',\n",
       "  '从中',\n",
       "  '从重',\n",
       "  '凑巧',\n",
       "  '粗',\n",
       "  '存心',\n",
       "  '达旦',\n",
       "  '打从',\n",
       "  '打开天窗说亮话',\n",
       "  '大',\n",
       "  '大不了',\n",
       "  '大大',\n",
       "  '大抵',\n",
       "  '大都',\n",
       "  '大多',\n",
       "  '大凡',\n",
       "  '大概',\n",
       "  '大家',\n",
       "  '大举',\n",
       "  '大略',\n",
       "  '大面儿上',\n",
       "  '大事',\n",
       "  '大体',\n",
       "  '大体上',\n",
       "  '大约',\n",
       "  '大张旗鼓',\n",
       "  '大致',\n",
       "  '呆呆地',\n",
       "  '带',\n",
       "  '殆',\n",
       "  '待到',\n",
       "  '单',\n",
       "  '单纯',\n",
       "  '单单',\n",
       "  '但愿',\n",
       "  '弹指之间',\n",
       "  '当场',\n",
       "  '当儿',\n",
       "  '当即',\n",
       "  '当口儿',\n",
       "  '当然',\n",
       "  '当庭',\n",
       "  '当头',\n",
       "  '当下',\n",
       "  '当真',\n",
       "  '当中',\n",
       "  '倒不如',\n",
       "  '倒不如说',\n",
       "  '倒是',\n",
       "  '到处',\n",
       "  '到底',\n",
       "  '到了儿',\n",
       "  '到目前为止',\n",
       "  '到头',\n",
       "  '到头来',\n",
       "  '得起',\n",
       "  '得天独厚',\n",
       "  '的确',\n",
       "  '等到',\n",
       "  '叮当',\n",
       "  '顶多',\n",
       "  '定',\n",
       "  '动不动',\n",
       "  '动辄',\n",
       "  '陡然',\n",
       "  '都',\n",
       "  '独',\n",
       "  '独自',\n",
       "  '断然',\n",
       "  '顿时',\n",
       "  '多次',\n",
       "  '多多',\n",
       "  '多多少少',\n",
       "  '多多益善',\n",
       "  '多亏',\n",
       "  '多年来',\n",
       "  '多年前',\n",
       "  '而后',\n",
       "  '而论',\n",
       "  '而又',\n",
       "  '尔等',\n",
       "  '二话不说',\n",
       "  '二话没说',\n",
       "  '反倒',\n",
       "  '反倒是',\n",
       "  '反而',\n",
       "  '反手',\n",
       "  '反之亦然',\n",
       "  '反之则',\n",
       "  '方',\n",
       "  '方才',\n",
       "  '方能',\n",
       "  '放量',\n",
       "  '非常',\n",
       "  '非得',\n",
       "  '分期',\n",
       "  '分期分批',\n",
       "  '分头',\n",
       "  '奋勇',\n",
       "  '愤然',\n",
       "  '风雨无阻',\n",
       "  '逢',\n",
       "  '弗',\n",
       "  '甫',\n",
       "  '嘎嘎',\n",
       "  '该当',\n",
       "  '概',\n",
       "  '赶快',\n",
       "  '赶早不赶晚',\n",
       "  '敢',\n",
       "  '敢情',\n",
       "  '敢于',\n",
       "  '刚',\n",
       "  '刚才',\n",
       "  '刚好',\n",
       "  '刚巧',\n",
       "  '高低',\n",
       "  '格外',\n",
       "  '隔日',\n",
       "  '隔夜',\n",
       "  '个人',\n",
       "  '各式',\n",
       "  '更',\n",
       "  '更加',\n",
       "  '更进一步',\n",
       "  '更为',\n",
       "  '公然',\n",
       "  '共',\n",
       "  '共总',\n",
       "  '够瞧的',\n",
       "  '姑且',\n",
       "  '古来',\n",
       "  '故而',\n",
       "  '故意',\n",
       "  '固',\n",
       "  '怪',\n",
       "  '怪不得',\n",
       "  '惯常',\n",
       "  '光',\n",
       "  '光是',\n",
       "  '归根到底',\n",
       "  '归根结底',\n",
       "  '过于',\n",
       "  '毫不',\n",
       "  '毫无',\n",
       "  '毫无保留地',\n",
       "  '毫无例外',\n",
       "  '好在',\n",
       "  '何必',\n",
       "  '何尝',\n",
       "  '何妨',\n",
       "  '何苦',\n",
       "  '何乐而不为',\n",
       "  '何须',\n",
       "  '何止',\n",
       "  '很',\n",
       "  '很多',\n",
       "  '很少',\n",
       "  '轰然',\n",
       "  '后来',\n",
       "  '呼啦',\n",
       "  '忽地',\n",
       "  '忽然',\n",
       "  '互',\n",
       "  '互相',\n",
       "  '哗啦',\n",
       "  '话说',\n",
       "  '还',\n",
       "  '恍然',\n",
       "  '会',\n",
       "  '豁然',\n",
       "  '活',\n",
       "  '伙同',\n",
       "  '或多或少',\n",
       "  '或许',\n",
       "  '基本',\n",
       "  '基本上',\n",
       "  '基于',\n",
       "  '极',\n",
       "  '极大',\n",
       "  '极度',\n",
       "  '极端',\n",
       "  '极力',\n",
       "  '极其',\n",
       "  '极为',\n",
       "  '急匆匆',\n",
       "  '即将',\n",
       "  '即刻',\n",
       "  '即是说',\n",
       "  '几度',\n",
       "  '几番',\n",
       "  '几乎',\n",
       "  '几经',\n",
       "  '既...又',\n",
       "  '继之',\n",
       "  '加上',\n",
       "  '加以',\n",
       "  '间或',\n",
       "  '简而言之',\n",
       "  '简言之',\n",
       "  '简直',\n",
       "  '见',\n",
       "  '将才',\n",
       "  '将近',\n",
       "  '将要',\n",
       "  '交口',\n",
       "  '较比',\n",
       "  '较为',\n",
       "  '接连不断',\n",
       "  '接下来',\n",
       "  '皆可',\n",
       "  '截然',\n",
       "  '截至',\n",
       "  '藉以',\n",
       "  '借此',\n",
       "  '借以',\n",
       "  '届时',\n",
       "  '仅',\n",
       "  '仅仅',\n",
       "  '谨',\n",
       "  ...],\n",
       " 'vect__strip_accents': None,\n",
       " 'vect__sublinear_tf': False,\n",
       " 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'vect__tokenizer': <function __main__.tokenizer>,\n",
       " 'vect__use_idf': True,\n",
       " 'vect__vocabulary': None}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 0])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /home/liuchao/.pyenv/versions/3.6.3/lib/python3.6/site-packages/jieba/dict.txt ...\n",
      "DEBUG:jieba:Building prefix dict from /home/liuchao/.pyenv/versions/3.6.3/lib/python3.6/site-packages/jieba/dict.txt ...\n",
      "Dumping model to file cache /tmp/jieba.u2cc073e987b6edce0cbd310765b52b7c.cache\n",
      "DEBUG:jieba:Dumping model to file cache /tmp/jieba.u2cc073e987b6edce0cbd310765b52b7c.cache\n",
      "Loading model cost 0.952 seconds.\n",
      "DEBUG:jieba:Loading model cost 0.952 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "DEBUG:jieba:Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'我 好 喜欢 你'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.23      0.88      0.36      6018\n",
      "   positive       0.99      0.85      0.92    120451\n",
      "\n",
      "avg / total       0.96      0.85      0.89    126469\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "class_name = [\"negative\", \"positive\"]\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred,target_names=class_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
